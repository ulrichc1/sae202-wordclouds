cooc <- t(m) %*% m
# Nettoyage du texte
corpus_clean <- tm_map(corpus, content_transformer(function(x) removePunctuation(x)))
corpus_clean <- tm_map(corpus_clean, content_transformer(function(x) removeNumbers(x)))
corpus_clean <- tm_map(corpus_clean, content_transformer(function(x) str_replace_all(x, "\\\\s+", " ")))
corpus_clean <- tm_map(corpus_clean, content_transformer(function(x) removeWords(x, stopwords("fr"))))
corpus_clean <- tm_map(corpus_clean, content_transformer(function(x) stemDocument(x)))
# Créer une matrice binaire de co-occurrences à partir du corpus nettoyé
dtm_clean <- DocumentTermMatrix(corpus_clean, control = list(wordLengths = c(3, Inf), bounds = list(global = c(10, Inf))))
cooc_clean <- t(as.matrix(dtm_clean)) %*% as.matrix(dtm_clean)
# Créer un objet igraph à partir de la matrice de co-occurrences nettoyée
g <- graph.adjacency(cooc_clean, mode = "undirected")
# Supprimer les mots les plus fréquents qui peuvent biaiser l'analyse
freqs <- rowSums(as.matrix(dtm_clean))
to_remove <- names(freqs[freqs > quantile(freqs, 0.9)])
g <- delete.vertices(g, to_remove)
freqs <- rowSums(as.matrix(dtm_clean))
to_remove <- names(freqs[freqs > quantile(freqs, 0.8)])
g <- delete.vertices(g, to_remove)
to_remove <- names(freqs[freqs > quantile(freqs, 0.5)])
to_remove <- names(freqs[freqs > quantile(freqs, 0.1)])
# Ulrich COUDIN - 07/03/2023
# SAE 2.02 - Exploration algorithmique d'un problème
require(pdftools) # lire les fichiers pdf
require(tm) #analyse text mining
require(wordcloud) # Nuage de mots
require(NLP)
require(RColorBrewer) # Gestion de couleurs
require(wordcloud2) # Nuage de mots
require(stringr)
require(SnowballC) # Stemming et StopWords
require(stopwords) #Stopwords
require(utf8) #Encodage
require(syuzhet)
require(lubridate)
require(ggplot2)
require(scales)
require(reshape2)
require(dplyr)
require(igraph)
require(ggraph)
require(widyr)
#------------------------------------------------------
#Récupération corpus de textes
files<-list.files(pattern = ".pdf") # listes de fichiers pdf par noms
allocutions<-lapply(files, pdf_text) # charge tous les fichiers pdf
length(allocutions) # verifie combien de fichiers ont ete charges
lapply(allocutions,length) #check la longueur de chaque fichier pdf
# Passage pdf en corpus txt
pdfdatabase <- Corpus(URISource(files), readerControl = list(reader = readPDF()))
#------------------------------------------------------
# nettoyage du contenu
# Matrice de données : Occurences d'apparitions de chaque mots dans chaque fichier PDF
# Seuls les mots qui apparaissent plus de 10 fois sont comptés
mots_a_retirer <- c("tant","tout","toute","toutes","trop","veux","chers","chac","les", "qui", "pour","cest","aussi","plus","tout","mais","ou","est","donc","or","ni","car","à","dans","par","pour","en","vers","avec","de","sans","sous","sur","d'abord","alors","depuis","deux","dont","fois","parce que","parce","sais","-elle", "-il", "10ème", "1er", "1ère", "2ème", "3ème", "4ème", "5ème", "6ème", "7ème", "8ème", "9ème", "a", "afin", "ai", "ainsi", "ais", "ait", "alors", "après", "as", "assez", "au", "aucun", "aucune", "auprès", "auquel", "auquelles", "auquels", "auraient", "aurais", "aurait", "aurez", "auriez", "aurions", "aurons", "auront", "aussi", "aussitôt", "autre", "autres", "aux", "avaient", "avais", "avait", "avant", "avec", "avez", "aviez", "avoir", "avons", "ayant", "beaucoup", "c'", "car", "ce", "ceci", "cela", "celle", "celles", "celui", "cependant", "certes", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "cinq", "comme", "d'", "d'abord", "dans", "de", "dehors", "delà", "depuis", "des", "dessous", "dessus", "deux", "deça", "dix", "doit", "donc", "dont", "du", "durant", "dès", "déjà", "elle", "elles", "en", "encore", "enfin", "entre", "er", "est", "est-ce", "et", "etc", "eu", "eurent", "eut", "faut", "fur", "hormis", "hors", "huit", "il", "ils", "j'", "je", "jusqu'", "l'", "la", "laquelle", "le", "lequel", "les", "lesquels", "leur", "leurs", "lors", "lorsque", "lui", "là", "m'", "mais", "malgré", "me", "melle", "mes", "mm", "mme", "moi", "moins", "mon", "mr", "même", "mêmes", "n'", "neuf", "ni", "non-", "nos", "notamment", "notre", "nous", "néanmoins", "nôtres", "on", "ont", "ou", "où", "par", "parce", "parfois", "parmi", "partout", "pas", "pendant", "peu", "peut", "peut-être", "plus", "plutôt", "pour", "pourquoi", "près", "puisqu'", "puisque", "qu'", "quand", "quant", "quatre", "que", "quel", "quelle", "quelles", "quelqu'", "quelque", "quelquefois", "quelques", "quels", "qui", "quoi", "quot", "s'", "sa", "sans", "se", "sept", "sera", "serai", "seraient", "serais", "serait", "seras", "serez", "seriez", "serions", "serons", "seront", "ses", "si", "sien", "siennes", "siens", "sitôt", "six", "soi", "sommes", "son", "sont", "sous", "souvent", "suis", "sur", "t'", "toi", "ton", "toujours", "tous", "tout", "toutefois", "toutes", "troiw", "tu", "un", "une", "unes", "uns", "voici", "voilà", "vos", "votre", "vous", "vôtres", "y", "à", "ème", "étaient", "étais", "était", "étant", "étiez", "étions", "êtes", "être", "afin", "ainsi", "alors", "après", "aucun", "aucune", "auprès", "auquel", "aussi", "autant", "aux", "avec", "car", "ceci", "cela", "celle", "celles", "celui", "cependant", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "comme", "comment", "dans", "des", "donc", "donné", "dont", "duquel", "dès", "déjà", "elle", "elles", "encore", "entre", "étant", "etc", "été", "eux", "furent", "grâce", "hors", "ici", "ils", "jusqu", "les", "leur", "leurs", "lors", "lui", "mais", "malgré", "mes") # mots à retirer
corpus <- tm_map(pdfdatabase, content_transformer(function(x) gsub("(l'|s'|n'|j'|qu'|d')", "", x))) # Supprime les articles avec une apostrophe
allocutions.tdm <- TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE,
stopwords = c(stopwords("fr"), mots_a_retirer), # stopwords en français + mots à retirer
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
stripWhitespace = TRUE,
bounds = list(global = c(10, Inf))))
inspect(allocutions.tdm[1:10,]) #Examine 10 mots
findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf) # Termes les plus fréquents
#------------------------------------------------------
# Analyse statistique
ft <- findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf)
as.matrix(allocutions.tdm[ft,]) #Fréquence (occurences) sous forme de matrice
ft.tdm <- as.matrix(allocutions.tdm[ft,]) # Somme de l'occurence de la fréquence de mots
v <- (sort(apply(ft.tdm, 1, sum), decreasing = TRUE))
d <- data.frame(word = names(v),freq=v)
#------------------------------------------------------
# Sauvegarde des résultats au format .csv (séparé par virgules)
dtm <- as.matrix(allocutions.tdm)
write.csv(dtm, file = "textmininganalysis.csv")
#------------------------------------------------------
# Représentations graphiques
# Histogramme
word_freq <- sort(rowSums(ft.tdm), decreasing = TRUE)
top_words <- names(word_freq)[1:10] # Sélectionne les 10 premiers mots les plus fréquents
barplot(word_freq[1:10], main = "Fréquence des mots", col = brewer.pal(8, "Spectral"), ylab = "Fréquence", xlab = "Mots", las = 2, cex.names = 0.6)
#------------------------------------------------------
# Nuage de mots
wordcloud(d$word,d$freq, random.order = FALSE, rot.per = 0.3, scale = c(3,.5),max.words = 105 ,colors = brewer.pal(8,"Dark2"))
# Nuage de mots dynamique
par(mar = rep(0, 4))
wordcloud2(data.frame(names = rownames(dtm), freq = rowSums(dtm)), size = 0.5, color = "random-light", fontFamily = "serif")
#------------------------------------------------------
# Score sentimental
#Lecture du fichier .csv crée précédemment
csvfile <- read.csv("textmininganalysis.csv", header = T)
sents <- iconv(csvfile)
s <- get_nrc_sentiment(sents) #Conversion pour analyse sentimentale
head(s)
# Histogramme
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
#keywording
# Convertir la matrice de termes en une matrice binaire de co-occurrence de termes
m <- as.matrix(allocutions.tdm)
m[m > 0] <- 1
cooc <- t(m) %*% m
# Nettoyage du texte
corpus_clean <- tm_map(corpus, content_transformer(function(x) removePunctuation(x)))
corpus_clean <- tm_map(corpus_clean, content_transformer(function(x) removeNumbers(x)))
corpus_clean <- tm_map(corpus_clean, content_transformer(function(x) str_replace_all(x, "\\\\s+", " ")))
corpus_clean <- tm_map(corpus_clean, content_transformer(function(x) removeWords(x, stopwords("fr"))))
corpus_clean <- tm_map(corpus_clean, content_transformer(function(x) stemDocument(x)))
# Créer une matrice binaire de co-occurrences à partir du corpus nettoyé
dtm_clean <- DocumentTermMatrix(corpus_clean, control = list(wordLengths = c(3, Inf), bounds = list(global = c(10, Inf))))
cooc_clean <- t(as.matrix(dtm_clean)) %*% as.matrix(dtm_clean)
# Créer un objet igraph à partir de la matrice de co-occurrences nettoyée
g <- graph.adjacency(cooc_clean, mode = "undirected")
# Supprimer les mots les plus fréquents qui peuvent biaiser l'analyse
freqs <- rowSums(as.matrix(dtm_clean))
to_remove <- names(freqs[freqs > quantile(freqs, 0.9)])
# Calculer les mesures de centralité pour chaque nœud
centrality <- centr_degree(g, mode = "total")
# Représenter le réseau de mots avec ggraph
ggraph(g) +
geom_edge_link(aes(width = weight), alpha = 0.8) +
geom_node_point(aes(size = centrality), color = "lightblue", alpha = 0.5) +
geom_node_text(aes(label = name), repel = TRUE) +
scale_size_continuous(range = c(5, 15)) +
theme_void()
gc()
# Ulrich COUDIN - 07/03/2023
# SAE 2.02 - Exploration algorithmique d'un problème
require(pdftools) # lire les fichiers pdf
require(tm) #analyse text mining
require(wordcloud) # Nuage de mots
require(NLP)
require(RColorBrewer) # Gestion de couleurs
require(wordcloud2) # Nuage de mots
require(stringr)
require(SnowballC) # Stemming et StopWords
require(stopwords) #Stopwords
require(utf8) #Encodage
require(syuzhet)
require(lubridate)
require(ggplot2)
require(scales)
require(reshape2)
require(dplyr)
require(igraph)
require(ggraph)
require(widyr)
#------------------------------------------------------
#Récupération corpus de textes
files<-list.files(pattern = ".pdf") # listes de fichiers pdf par noms
allocutions<-lapply(files, pdf_text) # charge tous les fichiers pdf
length(allocutions) # verifie combien de fichiers ont ete charges
lapply(allocutions,length) #check la longueur de chaque fichier pdf
# Passage pdf en corpus txt
pdfdatabase <- Corpus(URISource(files), readerControl = list(reader = readPDF()))
#------------------------------------------------------
# nettoyage du contenu
# Matrice de données : Occurences d'apparitions de chaque mots dans chaque fichier PDF
# Seuls les mots qui apparaissent plus de 10 fois sont comptés
mots_a_retirer <- c("tant","tout","toute","toutes","trop","veux","chers","chac","les", "qui", "pour","cest","aussi","plus","tout","mais","ou","est","donc","or","ni","car","à","dans","par","pour","en","vers","avec","de","sans","sous","sur","d'abord","alors","depuis","deux","dont","fois","parce que","parce","sais","-elle", "-il", "10ème", "1er", "1ère", "2ème", "3ème", "4ème", "5ème", "6ème", "7ème", "8ème", "9ème", "a", "afin", "ai", "ainsi", "ais", "ait", "alors", "après", "as", "assez", "au", "aucun", "aucune", "auprès", "auquel", "auquelles", "auquels", "auraient", "aurais", "aurait", "aurez", "auriez", "aurions", "aurons", "auront", "aussi", "aussitôt", "autre", "autres", "aux", "avaient", "avais", "avait", "avant", "avec", "avez", "aviez", "avoir", "avons", "ayant", "beaucoup", "c'", "car", "ce", "ceci", "cela", "celle", "celles", "celui", "cependant", "certes", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "cinq", "comme", "d'", "d'abord", "dans", "de", "dehors", "delà", "depuis", "des", "dessous", "dessus", "deux", "deça", "dix", "doit", "donc", "dont", "du", "durant", "dès", "déjà", "elle", "elles", "en", "encore", "enfin", "entre", "er", "est", "est-ce", "et", "etc", "eu", "eurent", "eut", "faut", "fur", "hormis", "hors", "huit", "il", "ils", "j'", "je", "jusqu'", "l'", "la", "laquelle", "le", "lequel", "les", "lesquels", "leur", "leurs", "lors", "lorsque", "lui", "là", "m'", "mais", "malgré", "me", "melle", "mes", "mm", "mme", "moi", "moins", "mon", "mr", "même", "mêmes", "n'", "neuf", "ni", "non-", "nos", "notamment", "notre", "nous", "néanmoins", "nôtres", "on", "ont", "ou", "où", "par", "parce", "parfois", "parmi", "partout", "pas", "pendant", "peu", "peut", "peut-être", "plus", "plutôt", "pour", "pourquoi", "près", "puisqu'", "puisque", "qu'", "quand", "quant", "quatre", "que", "quel", "quelle", "quelles", "quelqu'", "quelque", "quelquefois", "quelques", "quels", "qui", "quoi", "quot", "s'", "sa", "sans", "se", "sept", "sera", "serai", "seraient", "serais", "serait", "seras", "serez", "seriez", "serions", "serons", "seront", "ses", "si", "sien", "siennes", "siens", "sitôt", "six", "soi", "sommes", "son", "sont", "sous", "souvent", "suis", "sur", "t'", "toi", "ton", "toujours", "tous", "tout", "toutefois", "toutes", "troiw", "tu", "un", "une", "unes", "uns", "voici", "voilà", "vos", "votre", "vous", "vôtres", "y", "à", "ème", "étaient", "étais", "était", "étant", "étiez", "étions", "êtes", "être", "afin", "ainsi", "alors", "après", "aucun", "aucune", "auprès", "auquel", "aussi", "autant", "aux", "avec", "car", "ceci", "cela", "celle", "celles", "celui", "cependant", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "comme", "comment", "dans", "des", "donc", "donné", "dont", "duquel", "dès", "déjà", "elle", "elles", "encore", "entre", "étant", "etc", "été", "eux", "furent", "grâce", "hors", "ici", "ils", "jusqu", "les", "leur", "leurs", "lors", "lui", "mais", "malgré", "mes") # mots à retirer
corpus <- tm_map(pdfdatabase, content_transformer(function(x) gsub("(l'|s'|n'|j'|qu'|d')", "", x))) # Supprime les articles avec une apostrophe
allocutions.tdm <- TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE,
stopwords = c(stopwords("fr"), mots_a_retirer), # stopwords en français + mots à retirer
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
stripWhitespace = TRUE,
bounds = list(global = c(10, Inf))))
inspect(allocutions.tdm[1:10,]) #Examine 10 mots
findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf) # Termes les plus fréquents
#------------------------------------------------------
# Analyse statistique
ft <- findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf)
as.matrix(allocutions.tdm[ft,]) #Fréquence (occurences) sous forme de matrice
ft.tdm <- as.matrix(allocutions.tdm[ft,]) # Somme de l'occurence de la fréquence de mots
v <- (sort(apply(ft.tdm, 1, sum), decreasing = TRUE))
d <- data.frame(word = names(v),freq=v)
#------------------------------------------------------
# Sauvegarde des résultats au format .csv (séparé par virgules)
dtm <- as.matrix(allocutions.tdm)
write.csv(dtm, file = "textmininganalysis.csv")
#------------------------------------------------------
# Représentations graphiques
# Histogramme
word_freq <- sort(rowSums(ft.tdm), decreasing = TRUE)
top_words <- names(word_freq)[1:10] # Sélectionne les 10 premiers mots les plus fréquents
barplot(word_freq[1:10], main = "Fréquence des mots", col = brewer.pal(8, "Spectral"), ylab = "Fréquence", xlab = "Mots", las = 2, cex.names = 0.6)
#------------------------------------------------------
# Nuage de mots
wordcloud(d$word,d$freq, random.order = FALSE, rot.per = 0.3, scale = c(3,.5),max.words = 105 ,colors = brewer.pal(8,"Dark2"))
# Nuage de mots dynamique
par(mar = rep(0, 4))
wordcloud2(data.frame(names = rownames(dtm), freq = rowSums(dtm)), size = 0.5, color = "random-light", fontFamily = "serif")
#------------------------------------------------------
# Score sentimental
#Lecture du fichier .csv crée précédemment
csvfile <- read.csv("textmininganalysis.csv", header = T)
sents <- iconv(csvfile)
s <- get_nrc_sentiment(sents) #Conversion pour analyse sentimentale
head(s)
# Histogramme
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
# Ulrich COUDIN - 07/03/2023
# SAE 2.02 - Exploration algorithmique d'un problème
require(pdftools) # lire les fichiers pdf
require(tm) #analyse text mining
require(wordcloud) # Nuage de mots
require(NLP)
require(RColorBrewer) # Gestion de couleurs
require(wordcloud2) # Nuage de mots
require(stringr)
require(SnowballC) # Stemming et StopWords
require(stopwords) #Stopwords
require(utf8) #Encodage
require(syuzhet)
require(lubridate)
require(ggplot2)
require(scales)
require(reshape2)
require(dplyr)
#------------------------------------------------------
#Récupération corpus de textes
files<-list.files(pattern = ".pdf") # listes de fichiers pdf par noms
allocutions<-lapply(files, pdf_text) # charge tous les fichiers pdf
length(allocutions) # verifie combien de fichiers ont ete charges
lapply(allocutions,length) #check la longueur de chaque fichier pdf
# Passage pdf en corpus txt
pdfdatabase <- Corpus(URISource(files), readerControl = list(reader = readPDF()))
#------------------------------------------------------
# nettoyage du contenu
# Matrice de données : Occurences d'apparitions de chaque mots dans chaque fichier PDF
# Seuls les mots qui apparaissent plus de 10 fois sont comptés
mots_a_retirer <- c("tant","tout","toute","toutes","trop","veux","chers","chac","les", "qui", "pour","cest","aussi","plus","tout","mais","ou","est","donc","or","ni","car","à","dans","par","pour","en","vers","avec","de","sans","sous","sur","d'abord","alors","depuis","deux","dont","fois","parce que","parce","sais","-elle", "-il", "10ème", "1er", "1ère", "2ème", "3ème", "4ème", "5ème", "6ème", "7ème", "8ème", "9ème", "a", "afin", "ai", "ainsi", "ais", "ait", "alors", "après", "as", "assez", "au", "aucun", "aucune", "auprès", "auquel", "auquelles", "auquels", "auraient", "aurais", "aurait", "aurez", "auriez", "aurions", "aurons", "auront", "aussi", "aussitôt", "autre", "autres", "aux", "avaient", "avais", "avait", "avant", "avec", "avez", "aviez", "avoir", "avons", "ayant", "beaucoup", "c'", "car", "ce", "ceci", "cela", "celle", "celles", "celui", "cependant", "certes", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "cinq", "comme", "d'", "d'abord", "dans", "de", "dehors", "delà", "depuis", "des", "dessous", "dessus", "deux", "deça", "dix", "doit", "donc", "dont", "du", "durant", "dès", "déjà", "elle", "elles", "en", "encore", "enfin", "entre", "er", "est", "est-ce", "et", "etc", "eu", "eurent", "eut", "faut", "fur", "hormis", "hors", "huit", "il", "ils", "j'", "je", "jusqu'", "l'", "la", "laquelle", "le", "lequel", "les", "lesquels", "leur", "leurs", "lors", "lorsque", "lui", "là", "m'", "mais", "malgré", "me", "melle", "mes", "mm", "mme", "moi", "moins", "mon", "mr", "même", "mêmes", "n'", "neuf", "ni", "non-", "nos", "notamment", "notre", "nous", "néanmoins", "nôtres", "on", "ont", "ou", "où", "par", "parce", "parfois", "parmi", "partout", "pas", "pendant", "peu", "peut", "peut-être", "plus", "plutôt", "pour", "pourquoi", "près", "puisqu'", "puisque", "qu'", "quand", "quant", "quatre", "que", "quel", "quelle", "quelles", "quelqu'", "quelque", "quelquefois", "quelques", "quels", "qui", "quoi", "quot", "s'", "sa", "sans", "se", "sept", "sera", "serai", "seraient", "serais", "serait", "seras", "serez", "seriez", "serions", "serons", "seront", "ses", "si", "sien", "siennes", "siens", "sitôt", "six", "soi", "sommes", "son", "sont", "sous", "souvent", "suis", "sur", "t'", "toi", "ton", "toujours", "tous", "tout", "toutefois", "toutes", "troiw", "tu", "un", "une", "unes", "uns", "voici", "voilà", "vos", "votre", "vous", "vôtres", "y", "à", "ème", "étaient", "étais", "était", "étant", "étiez", "étions", "êtes", "être", "afin", "ainsi", "alors", "après", "aucun", "aucune", "auprès", "auquel", "aussi", "autant", "aux", "avec", "car", "ceci", "cela", "celle", "celles", "celui", "cependant", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "comme", "comment", "dans", "des", "donc", "donné", "dont", "duquel", "dès", "déjà", "elle", "elles", "encore", "entre", "étant", "etc", "été", "eux", "furent", "grâce", "hors", "ici", "ils", "jusqu", "les", "leur", "leurs", "lors", "lui", "mais", "malgré", "mes") # mots à retirer
corpus <- tm_map(pdfdatabase, content_transformer(function(x) gsub("(l'|s'|n'|j'|qu'|d')", "", x))) # Supprime les articles avec une apostrophe
allocutions.tdm <- TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE,
stopwords = c(stopwords("fr"), mots_a_retirer), # stopwords en français + mots à retirer
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
stripWhitespace = TRUE,
bounds = list(global = c(10, Inf))))
inspect(allocutions.tdm[1:10,]) #Examine 10 mots
findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf) # Termes les plus fréquents
#------------------------------------------------------
# Analyse statistique
ft <- findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf)
as.matrix(allocutions.tdm[ft,]) #Fréquence (occurences) sous forme de matrice
ft.tdm <- as.matrix(allocutions.tdm[ft,]) # Somme de l'occurence de la fréquence de mots
v <- (sort(apply(ft.tdm, 1, sum), decreasing = TRUE))
d <- data.frame(word = names(v),freq=v)
#------------------------------------------------------
# Sauvegarde des résultats au format .csv (séparé par virgules)
dtm <- as.matrix(allocutions.tdm)
write.csv(dtm, file = "textmininganalysis.csv")
#------------------------------------------------------
# Représentations graphiques
# Histogramme
word_freq <- sort(rowSums(ft.tdm), decreasing = TRUE)
top_words <- names(word_freq)[1:10] # Sélectionne les 10 premiers mots les plus fréquents
barplot(word_freq[1:10], main = "Fréquence des mots", col = brewer.pal(8, "Spectral"), ylab = "Fréquence", xlab = "Mots", las = 2, cex.names = 0.6)
#------------------------------------------------------
# Nuage de mots
wordcloud(d$word,d$freq, random.order = FALSE, rot.per = 0.3, scale = c(3,.5),max.words = 105 ,colors = brewer.pal(8,"Dark2"))
# Nuage de mots dynamique
par(mar = rep(0, 4))
wordcloud2(data.frame(names = rownames(dtm), freq = rowSums(dtm)), size = 0.5, color = "random-light", fontFamily = "serif")
#------------------------------------------------------
# Score sentimental
#Lecture du fichier .csv crée précédemment
csvfile <- read.csv("textmininganalysis.csv", header = T)
sents <- iconv(csvfile)
s <- get_nrc_sentiment(sents) #Conversion pour analyse sentimentale
head(s)
# Histogramme
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
# Histogramme
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
wordcloud(d$word,d$freq, random.order = FALSE, rot.per = 0.3, scale = c(3,.5),max.words = 105 ,colors = brewer.pal(8,"Dark2"))
barplot(word_freq[1:10], main = "Fréquence des mots", col = brewer.pal(8, "Spectral"), ylab = "Fréquence", xlab = "Mots", las = 2, cex.names = 0.6)
# Ulrich COUDIN - 07/03/2023
# SAE 2.02 - Exploration algorithmique d'un problème
require(pdftools) # lire les fichiers pdf
require(tm) #analyse text mining
require(wordcloud) # Nuage de mots
require(NLP)
require(RColorBrewer) # Gestion de couleurs
require(wordcloud2) # Nuage de mots
require(stringr)
require(SnowballC) # Stemming et StopWords
require(stopwords) #Stopwords
require(utf8) #Encodage
require(syuzhet)
require(lubridate)
require(ggplot2)
require(scales)
require(reshape2)
require(dplyr)
#------------------------------------------------------
#Récupération corpus de textes
files<-list.files(pattern = ".pdf") # listes de fichiers pdf par noms
allocutions<-lapply(files, pdf_text) # charge tous les fichiers pdf
length(allocutions) # verifie combien de fichiers ont ete charges
lapply(allocutions,length) #check la longueur de chaque fichier pdf
# Passage pdf en corpus txt
pdfdatabase <- Corpus(URISource(files), readerControl = list(reader = readPDF()))
#------------------------------------------------------
# nettoyage du contenu
# Matrice de données : Occurences d'apparitions de chaque mots dans chaque fichier PDF
# Seuls les mots qui apparaissent plus de 10 fois sont comptés
mots_a_retirer <- c("tant","tout","toute","toutes","trop","veux","chers","chac","les", "qui", "pour","cest","aussi","plus","tout","mais","ou","est","donc","or","ni","car","à","dans","par","pour","en","vers","avec","de","sans","sous","sur","d'abord","alors","depuis","deux","dont","fois","parce que","parce","sais","-elle", "-il", "10ème", "1er", "1ère", "2ème", "3ème", "4ème", "5ème", "6ème", "7ème", "8ème", "9ème", "a", "afin", "ai", "ainsi", "ais", "ait", "alors", "après", "as", "assez", "au", "aucun", "aucune", "auprès", "auquel", "auquelles", "auquels", "auraient", "aurais", "aurait", "aurez", "auriez", "aurions", "aurons", "auront", "aussi", "aussitôt", "autre", "autres", "aux", "avaient", "avais", "avait", "avant", "avec", "avez", "aviez", "avoir", "avons", "ayant", "beaucoup", "c'", "car", "ce", "ceci", "cela", "celle", "celles", "celui", "cependant", "certes", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "cinq", "comme", "d'", "d'abord", "dans", "de", "dehors", "delà", "depuis", "des", "dessous", "dessus", "deux", "deça", "dix", "doit", "donc", "dont", "du", "durant", "dès", "déjà", "elle", "elles", "en", "encore", "enfin", "entre", "er", "est", "est-ce", "et", "etc", "eu", "eurent", "eut", "faut", "fur", "hormis", "hors", "huit", "il", "ils", "j'", "je", "jusqu'", "l'", "la", "laquelle", "le", "lequel", "les", "lesquels", "leur", "leurs", "lors", "lorsque", "lui", "là", "m'", "mais", "malgré", "me", "melle", "mes", "mm", "mme", "moi", "moins", "mon", "mr", "même", "mêmes", "n'", "neuf", "ni", "non-", "nos", "notamment", "notre", "nous", "néanmoins", "nôtres", "on", "ont", "ou", "où", "par", "parce", "parfois", "parmi", "partout", "pas", "pendant", "peu", "peut", "peut-être", "plus", "plutôt", "pour", "pourquoi", "près", "puisqu'", "puisque", "qu'", "quand", "quant", "quatre", "que", "quel", "quelle", "quelles", "quelqu'", "quelque", "quelquefois", "quelques", "quels", "qui", "quoi", "quot", "s'", "sa", "sans", "se", "sept", "sera", "serai", "seraient", "serais", "serait", "seras", "serez", "seriez", "serions", "serons", "seront", "ses", "si", "sien", "siennes", "siens", "sitôt", "six", "soi", "sommes", "son", "sont", "sous", "souvent", "suis", "sur", "t'", "toi", "ton", "toujours", "tous", "tout", "toutefois", "toutes", "troiw", "tu", "un", "une", "unes", "uns", "voici", "voilà", "vos", "votre", "vous", "vôtres", "y", "à", "ème", "étaient", "étais", "était", "étant", "étiez", "étions", "êtes", "être", "afin", "ainsi", "alors", "après", "aucun", "aucune", "auprès", "auquel", "aussi", "autant", "aux", "avec", "car", "ceci", "cela", "celle", "celles", "celui", "cependant", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "comme", "comment", "dans", "des", "donc", "donné", "dont", "duquel", "dès", "déjà", "elle", "elles", "encore", "entre", "étant", "etc", "été", "eux", "furent", "grâce", "hors", "ici", "ils", "jusqu", "les", "leur", "leurs", "lors", "lui", "mais", "malgré", "mes") # mots à retirer
corpus <- tm_map(pdfdatabase, content_transformer(function(x) gsub("(l'|s'|n'|j'|qu'|d')", "", x))) # Supprime les articles avec une apostrophe
allocutions.tdm <- TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE,
stopwords = c(stopwords("fr"), mots_a_retirer), # stopwords en français + mots à retirer
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
stripWhitespace = TRUE,
bounds = list(global = c(10, Inf))))
inspect(allocutions.tdm[1:10,]) #Examine 10 mots
findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf) # Termes les plus fréquents
#------------------------------------------------------
# Analyse statistique
ft <- findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf)
as.matrix(allocutions.tdm[ft,]) #Fréquence (occurences) sous forme de matrice
ft.tdm <- as.matrix(allocutions.tdm[ft,]) # Somme de l'occurence de la fréquence de mots
v <- (sort(apply(ft.tdm, 1, sum), decreasing = TRUE))
d <- data.frame(word = names(v),freq=v)
#------------------------------------------------------
# Sauvegarde des résultats au format .csv (séparé par virgules)
dtm <- as.matrix(allocutions.tdm)
write.csv(dtm, file = "textmininganalysis.csv")
#------------------------------------------------------
# Représentations graphiques
# Histogramme
word_freq <- sort(rowSums(ft.tdm), decreasing = TRUE)
top_words <- names(word_freq)[1:10] # Sélectionne les 10 premiers mots les plus fréquents
barplot(word_freq[1:10], main = "Fréquence des mots", col = brewer.pal(8, "Spectral"), ylab = "Fréquence", xlab = "Mots", las = 2, cex.names = 0.6)
#------------------------------------------------------
# Nuage de mots
wordcloud(d$word,d$freq, random.order = FALSE, rot.per = 0.3, scale = c(3,.5),max.words = 105 ,colors = brewer.pal(8,"Dark2"))
# Nuage de mots dynamique
par(mar = rep(0, 4))
wordcloud2(data.frame(names = rownames(dtm), freq = rowSums(dtm)), size = 0.5, color = "random-light", fontFamily = "serif")
#------------------------------------------------------
# Score sentimental
#Lecture du fichier .csv crée précédemment
csvfile <- read.csv("textmininganalysis.csv", header = T)
sents <- iconv(csvfile)
s <- get_nrc_sentiment(sents) #Conversion pour analyse sentimentale
head(s)
# Histogramme
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
csvfile <- read.csv("textmininganalysis.csv", header = T)
sents <- iconv(csvfile)
s <- get_nrc_sentiment(sents) #Conversion pour analyse sentimentale
head(s)
# Histogramme
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
pdfdatabase
corpus
allocutions.tdm
allocutions
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
# Ulrich COUDIN - 07/03/2023
# SAE 2.02 - Exploration algorithmique d'un problème
require(pdftools) # lire les fichiers pdf
require(tm) #analyse text mining
require(wordcloud) # Nuage de mots
require(NLP)
require(RColorBrewer) # Gestion de couleurs
require(wordcloud2) # Nuage de mots
require(stringr)
require(SnowballC) # Stemming et StopWords
require(stopwords) #Stopwords
require(utf8) #Encodage
require(syuzhet)
require(lubridate)
require(ggplot2)
require(scales)
require(reshape2)
require(dplyr)
#------------------------------------------------------
#Récupération corpus de textes
files<-list.files(pattern = ".pdf") # listes de fichiers pdf par noms
allocutions<-lapply(files, pdf_text) # charge tous les fichiers pdf
length(allocutions) # verifie combien de fichiers ont ete charges
lapply(allocutions,length) #check la longueur de chaque fichier pdf
# Passage pdf en corpus txt
pdfdatabase <- Corpus(URISource(files), readerControl = list(reader = readPDF()))
#------------------------------------------------------
# nettoyage du contenu
# Matrice de données : Occurences d'apparitions de chaque mots dans chaque fichier PDF
# Seuls les mots qui apparaissent plus de 10 fois sont comptés
mots_a_retirer <- c("d'abord","tant","tout","toute","toutes","trop","veux","chers","chac","les", "qui", "pour","cest","aussi","plus","tout","mais","ou","est","donc","or","ni","car","à","dans","par","pour","en","vers","avec","de","sans","sous","sur","d'abord","alors","depuis","deux","dont","fois","parce que","parce","sais","-elle", "-il", "10ème", "1er", "1ère", "2ème", "3ème", "4ème", "5ème", "6ème", "7ème", "8ème", "9ème", "a", "afin", "ai", "ainsi", "ais", "ait", "alors", "après", "as", "assez", "au", "aucun", "aucune", "auprès", "auquel", "auquelles", "auquels", "auraient", "aurais", "aurait", "aurez", "auriez", "aurions", "aurons", "auront", "aussi", "aussitôt", "autre", "autres", "aux", "avaient", "avais", "avait", "avant", "avec", "avez", "aviez", "avoir", "avons", "ayant", "beaucoup", "c'", "car", "ce", "ceci", "cela", "celle", "celles", "celui", "cependant", "certes", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "cinq", "comme", "d'", "d'abord", "dans", "de", "dehors", "delà", "depuis", "des", "dessous", "dessus", "deux", "deça", "dix", "doit", "donc", "dont", "du", "durant", "dès", "déjà", "elle", "elles", "en", "encore", "enfin", "entre", "er", "est", "est-ce", "et", "etc", "eu", "eurent", "eut", "faut", "fur", "hormis", "hors", "huit", "il", "ils", "j'", "je", "jusqu'", "l'", "la", "laquelle", "le", "lequel", "les", "lesquels", "leur", "leurs", "lors", "lorsque", "lui", "là", "m'", "mais", "malgré", "me", "melle", "mes", "mm", "mme", "moi", "moins", "mon", "mr", "même", "mêmes", "n'", "neuf", "ni", "non-", "nos", "notamment", "notre", "nous", "néanmoins", "nôtres", "on", "ont", "ou", "où", "par", "parce", "parfois", "parmi", "partout", "pas", "pendant", "peu", "peut", "peut-être", "plus", "plutôt", "pour", "pourquoi", "près", "puisqu'", "puisque", "qu'", "quand", "quant", "quatre", "que", "quel", "quelle", "quelles", "quelqu'", "quelque", "quelquefois", "quelques", "quels", "qui", "quoi", "quot", "s'", "sa", "sans", "se", "sept", "sera", "serai", "seraient", "serais", "serait", "seras", "serez", "seriez", "serions", "serons", "seront", "ses", "si", "sien", "siennes", "siens", "sitôt", "six", "soi", "sommes", "son", "sont", "sous", "souvent", "suis", "sur", "t'", "toi", "ton", "toujours", "tous", "tout", "toutefois", "toutes", "troiw", "tu", "un", "une", "unes", "uns", "voici", "voilà", "vos", "votre", "vous", "vôtres", "y", "à", "ème", "étaient", "étais", "était", "étant", "étiez", "étions", "êtes", "être", "afin", "ainsi", "alors", "après", "aucun", "aucune", "auprès", "auquel", "aussi", "autant", "aux", "avec", "car", "ceci", "cela", "celle", "celles", "celui", "cependant", "ces", "cet", "cette", "ceux", "chacun", "chacune", "chaque", "chez", "comme", "comment", "dans", "des", "donc", "donné", "dont", "duquel", "dès", "déjà", "elle", "elles", "encore", "entre", "étant", "etc", "été", "eux", "furent", "grâce", "hors", "ici", "ils", "jusqu", "les", "leur", "leurs", "lors", "lui", "mais", "malgré", "mes") # mots à retirer
corpus <- tm_map(pdfdatabase, content_transformer(function(x) gsub("(l'|s'|n'|j'|qu'|d')", "", x))) # Supprime les articles avec une apostrophe
allocutions.tdm <- TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE,
stopwords = c(stopwords("fr"), mots_a_retirer), # stopwords en français + mots à retirer
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
stripWhitespace = TRUE,
bounds = list(global = c(10, Inf))))
inspect(allocutions.tdm[1:10,]) #Examine 10 mots
findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf) # Termes les plus fréquents
#------------------------------------------------------
# Analyse statistique
ft <- findFreqTerms(allocutions.tdm, lowfreq = 30, highfreq = Inf)
as.matrix(allocutions.tdm[ft,]) #Fréquence (occurences) sous forme de matrice
ft.tdm <- as.matrix(allocutions.tdm[ft,]) # Somme de l'occurence de la fréquence de mots
v <- (sort(apply(ft.tdm, 1, sum), decreasing = TRUE))
d <- data.frame(word = names(v),freq=v)
#------------------------------------------------------
# Sauvegarde des résultats au format .csv (séparé par virgules)
dtm <- as.matrix(allocutions.tdm)
write.csv(dtm, file = "textmininganalysis.csv")
#------------------------------------------------------
# Représentations graphiques
# Histogramme
word_freq <- sort(rowSums(ft.tdm), decreasing = TRUE)
top_words <- names(word_freq)[1:10] # Sélectionne les 10 premiers mots les plus fréquents
barplot(word_freq[1:10], main = "Fréquence des mots", col = brewer.pal(8, "Spectral"), ylab = "Fréquence", xlab = "Mots", las = 2, cex.names = 0.6)
#------------------------------------------------------
# Nuage de mots
wordcloud(d$word,d$freq, random.order = FALSE, rot.per = 0.3, scale = c(3,.5),max.words = 105 ,colors = brewer.pal(8,"Dark2"))
# Nuage de mots dynamique
par(mar = rep(0, 4))
wordcloud2(data.frame(names = rownames(dtm), freq = rowSums(dtm)), size = 0.5, color = "random-light", fontFamily = "serif")
#------------------------------------------------------
# Score sentimental
#Lecture du fichier .csv crée précédemment
csvfile <- read.csv("textmininganalysis.csv", header = T)
sents <- iconv(csvfile)
s <- get_nrc_sentiment(sents) #Conversion pour analyse sentimentale
head(s)
# Histogramme
barplot(colSums(s),
las = 2,
col = rainbow(10),
xlab = 'Sentiments',
ylab = 'Fréquence',
main = 'Score sentimental',
cex.names = 0.6)
